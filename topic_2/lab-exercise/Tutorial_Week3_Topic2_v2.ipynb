{"cells": [{"cell_type": "markdown", "metadata": {"id": "SMgqAFAKEhC3"}, "source": ["# **Week 3 - Topic 2 - Data Processing**\n", "**Learning outcomes**\n", "1. Dealing with missing data\n", "2. Handling categorical data\n", "3. Selecting meaningful features\n", "4. Assessing feature importance with Random Forests"]}, {"cell_type": "markdown", "metadata": {"id": "VswDZYoFEyQB"}, "source": ["# 1 Dealing with missing data\n", "## 1.1 Identifying missing values in tabular data\n", "Let's create a simple example ***dataframe*** from a CSV file to get a better grasp of the problem:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "E47LYZwCExOT", "outputId": "65106564-da16-4160-d039-2a56f9710bcb"}, "outputs": [], "source": ["import pandas as pd\n", "from io import StringIO\n", "import sys\n", "csv_data = \\\n", "'''A,B,C,D\n", "1.0,2.0,3.0,4.0\n", "5.0,6.0,,8.0\n", "10.0,11.0,12.0,'''\n", "df = pd.read_csv(StringIO(csv_data))\n", "df"]}, {"cell_type": "markdown", "metadata": {"id": "YwW-m_zpFZJh"}, "source": ["***Query:*** *What do you observe?* <br>\n", "**Missing Value** <br>\n", "Can you count them? Yes"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "lS_ixkHsFYv_", "outputId": "ea9d74a5-8b56-4b70-8bbb-5a7cdc7a57d8"}, "outputs": [], "source": ["df.isnull().sum()"]}, {"cell_type": "markdown", "metadata": {"id": "uJGEym-jGavv"}, "source": ["## 1.2 Eliminating samples or features with missing values\n", "The simplest way to deal with missing data is to remove the corresponding features (columns) or samples (rows) from the dataset. <br>\n", "To drop the rows or columns with missing values, we can use *dropna* method:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 81}, "id": "pDJXNAdDGSbZ", "outputId": "9d004668-635b-453e-850f-bb60f4ab3e60"}, "outputs": [], "source": ["df.dropna(axis=0) # axis = 0 for rows"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "wspckTKNHLqL", "outputId": "5f5575c7-8572-460b-c207-cf8c0e379f46"}, "outputs": [], "source": ["df.dropna(axis=1) # axis = 1 for columns"]}, {"cell_type": "markdown", "metadata": {"id": "mfSpszOkHdFk"}, "source": ["We can also try to only drop rows where all columns are NaN"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "_C5R8Oz5HgfS", "outputId": "a1ee20d9-a5a4-4aa2-deac-0639926e76c6"}, "outputs": [], "source": ["df.dropna(how='all')"]}, {"cell_type": "markdown", "metadata": {"id": "5Af5XBFUHmHk"}, "source": ["note: You see the whole array is returned here since we don't have a row with where all values are *NaN*.\n", "Try to only drop rows where NaN appear in specific columns (here: 'C')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 112}, "id": "Uazc2UHfH7oH", "outputId": "a27398a1-d509-47b3-f173-d386b09968dd"}, "outputs": [], "source": ["df.dropna(subset=['C'])"]}, {"cell_type": "markdown", "metadata": {"id": "SKmeETZ6IjPd"}, "source": ["## 1.3 Imputing missing values\n", "The removal of samples or dropping of feature columns is simple, but we might lose too much\n", "valuable data. In this case, we can use different interpolation techniques to estimate the missing\n", "values from the other training samples in our datasets. <br>\n", "One common interpolation technique is *mean* imputation, where we simply replace the missing values with the *mean* value of the entire feature column. <br>\n", "To achieve this, we can use ***simpleImputer*** class from scikit-learn:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ylsL5ntxI7G0", "outputId": "42a8bce4-9fe1-4156-9916-c3bdb26a7cbd"}, "outputs": [], "source": ["df.values"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "09kiAmnYI-8I", "outputId": "f6a1c0c0-7278-4867-c7b0-ad3ce74d1419"}, "outputs": [], "source": ["from sklearn.impute import SimpleImputer\n", "import numpy as np\n", "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n", "imr = imr.fit(df.values)\n", "imputed_data = imr.transform(df.values)\n", "imputed_data"]}, {"cell_type": "markdown", "metadata": {"id": "a5STkbfeJnPe"}, "source": ["**Do:** Try other options for the strategy parameter 'median' or 'most_frequent'. The latter one replaces the missing values with the most frequent values, which is useful for imputing categorical feature values."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "DDtJlg76J5-B", "outputId": "1f33482e-fb79-409b-876c-b1915b4860ef"}, "outputs": [], "source": ["imr2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n", "imr2 = imr2.fit(df.values)\n", "imputed_data = imr2.transform(df.values)\n", "imputed_data"]}, {"cell_type": "markdown", "metadata": {"id": "3FzDUsr1Kf4t"}, "source": ["# Handling categorical data\n", "## 2.1 Nominal and ordinal features\n", "Let's create a new DataFrame,"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "JcgieWIrKyvs", "outputId": "f9a3738d-ee36-4f48-f0ec-cc81a4f43e28"}, "outputs": [], "source": ["import pandas as pd\n", "df = pd.DataFrame([['green', 'M', 10.1, 'class1'],\n", "                    ['red', 'L', 13.5, 'class2'],\n", "                    ['blue', 'XL', 15.3, 'class1']])\n", "df.columns = ['color', 'size', 'price', 'classlabel']\n", "df"]}, {"cell_type": "markdown", "metadata": {"id": "PTjCOmTiKmTg"}, "source": ["**Find** type of features:  nominal and ordinal? Why?\n", "\n", "nominal feature - color; ordinal feature - size; numerical feature - price"]}, {"cell_type": "markdown", "metadata": {"id": "Ier7wDoZM8ug"}, "source": ["## 2.2 Mapping ordinal features\n", "To make sure the learning algorithm interprets the ordinal features correctly, we need to convert the categorical string values into integers."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "tGVddxWsNC0D", "outputId": "4b1bc3fe-7de0-42fc-fdf3-141b3ad932ef"}, "outputs": [], "source": ["size_mapping = {'XL': 3, 'L': 2, 'M': 1}\n", "df['size'] = df['size'].map(size_mapping)\n", "df"]}, {"cell_type": "markdown", "metadata": {"id": "H0c0ImgrNdda"}, "source": ["## 2.3 Encoding class labels\n", "Many machine learning libraries requires that class labels are encoded as integer values. <br>\n", "Although most estimators for classification in scikit-learn convert class labels to integers internally, it is\n", "considered good practice to provide class labels as integer arrays to avoid technical glitches. <br>\n", "To achieve this, one way to do is to use the convenient LabelEncoder class directly implemented in scikit-learn.\n", "<br> Note here the *fit_transform* method is just a shortcut for calling fit and transform separately."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8lwtxotfN5Su", "outputId": "eb7362cc-8669-4b9c-cf98-6492a7de8c4f"}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n", "class_le = LabelEncoder()\n", "y = class_le.fit_transform(df['classlabel'].values)\n", "y"]}, {"cell_type": "markdown", "metadata": {"id": "hxlPpAXaOKUf"}, "source": ["We can use the inverse_transform method to transform the integer class labels back into their original string representation."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "D4vTwZz2OTxu", "outputId": "5fceaac0-49fb-406b-ed77-425c764f905a"}, "outputs": [], "source": ["# reverse mapping\n", "class_le.inverse_transform(y)"]}, {"cell_type": "markdown", "metadata": {"id": "wYZ0xpP2OgUI"}, "source": ["## 2.4 Performing one-hot encoding on nominal features\n", "We could use a similar approach to transform the nominal color column of the dataset:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "GtoyALaAOmcl", "outputId": "6c41424d-9d6d-4144-f830-0c58c8ac1956"}, "outputs": [], "source": ["X = df[['color', 'size', 'price']].values\n", "color_le = LabelEncoder()\n", "X[:, 0] = color_le.fit_transform(X[:, 0])\n", "X"]}, {"cell_type": "markdown", "metadata": {"id": "7DNeBYytPsYh"}, "source": ["You can see that the feature 'color' is encoded as follows:\n", "blue = 0, green = 1, red = 2 <br>\n", "If we stop at this point and feed the array to our classifer, we will make a mistake. Because the learning algorithm will assume that green is larger than blue, and red is larger than green. This may result in sub-optimal results.\n", "\n", "<br>\n", "To solve this issue, we can use a technique called one-hot encoding. It creates a new dummy feature for each unique value in the nominal feature column. We can use the OneHotEncoder that is implemented in scikit-learn's preprocessing module:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "hThZtJhDQqrn", "outputId": "b39e6544-4228-44c4-9550-a7d5264bb112"}, "outputs": [], "source": ["from sklearn.preprocessing import OneHotEncoder\n", "X = df[['color', 'size', 'price']].values\n", "color_ohe = OneHotEncoder()\n", "color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()"]}, {"cell_type": "markdown", "metadata": {"id": "h5bT6bvbQyjQ"}, "source": ["A more convenient way to create dummy features via one-hot encoding is to use the *get_dummies* method implemented in pandas. <br>\n", "*get_dummies* method will only convert string columns and leave all other columns unchanged:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "f1Y_wEsDQ9be", "outputId": "5fb0738e-60d6-4abc-9de3-40c0805461d8"}, "outputs": [], "source": ["pd.get_dummies(df[['price', 'color', 'size']])"]}, {"cell_type": "markdown", "metadata": {"id": "LP6Cr4Q_RDVm"}, "source": ["Keep in mind that it introduces multicollinearity, which can be an issue for certain methods (e.g. methods that require matrix inversion). If features are highly correlated, matrices are computationally difficult to invert, which can lead to numerically unstable estimates. <br>\n", "To reduce the correlation among variables, we can simply remove one feature column from the one-hot encoded array. By doing so, we do not lose any important information by removing a feature column.\n", "We can drop the first column by passing a True argument to the drop_first parameter:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "hKeJ_tCMR72R", "outputId": "f0c38270-6076-40ac-f7b2-a8bd93a0e027"}, "outputs": [], "source": ["# multicollinearity guard in get_dummies\n", "pd.get_dummies(df[['price', 'color', 'size']], drop_first=True)"]}, {"cell_type": "markdown", "metadata": {"id": "l5SG3YZsR6cL"}, "source": ["# 3 Partitioning a dataset into a seperate training and test set\n", "Let's prepare a new dataset, the Wine dataset from the UCI."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 241}, "id": "0l5AbevJSM3c", "outputId": "a7a68800-53eb-4da7-e5af-1dcfa295df62"}, "outputs": [], "source": ["df_wine = pd.read_csv('https://archive.ics.uci.edu/'\n", "'ml/machine-learning-databases/wine/wine.data',\n", "header=None)\n", "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n", "'Alcalinity of ash', 'Magnesium', 'Total phenols',\n", "'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n", "'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n", "'Proline']\n", "print('Class labels', np.unique(df_wine['Class label']))\n", "df_wine.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "9gqk8b58SvSo"}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n", "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"]}, {"cell_type": "markdown", "metadata": {"id": "7-83fxeSS_I1"}, "source": ["# 4 Bringing features onto the same scale\n", "Two common approaches to bring different features onto the same scale:\n", "<br>\n", "normalization - rescale features to a range of [0, 1]; a special case of min-max scaling\n", "<br>\n", "standardization - center the feature columns at mean 0 with standard deviation 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "-SiKoWZ-TLZN"}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n", "mms = MinMaxScaler()\n", "X_train_norm = mms.fit_transform(X_train)\n", "X_test_norm = mms.transform(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "KQELnlkeTPYl"}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "stdsc = StandardScaler()\n", "X_train_std = stdsc.fit_transform(X_train)\n", "X_test_std = stdsc.transform(X_test)"]}, {"cell_type": "markdown", "metadata": {"id": "2Vhgyz4WTk6j"}, "source": ["# 5 electing meaningful features\n", "L1 and L2 regularization as penalties against model complexity\n", "Sparse solutions with L1-regularization\n", "\n", "For regularized models in scikit-learn that support L1 regularization, we can simply set the penalty\n", "parameter to 'l1' to obtain a sparse solution:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 74}, "id": "LMaLufT2T_Yd", "outputId": "e51e47e3-195f-4a1f-d69c-d5e86589da60"}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "LogisticRegression(penalty='l1')"]}, {"cell_type": "markdown", "metadata": {"id": "naea8-c-UET3"}, "source": ["Apply it to the standardized Wine data: Note that C=1.0 is the default. You can increase or\n", "decrease it to make the regulariztion effect stronger or weaker, respectively."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "H6huH9yGUHmN", "outputId": "70509fb0-14a8-4ffb-a875-e559525c6805"}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "lr = LogisticRegression(penalty='l1', C=1.0, solver='liblinear', multi_class='ovr')\n", "lr.fit(X_train_std, y_train)\n", "print('Training accuracy:', lr.score(X_train_std, y_train))\n", "print('Test accuracy:', lr.score(X_test_std, y_test))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ntWcXR-DUxMY", "outputId": "3db6b398-20d4-4f85-8724-a923a36e0a05"}, "outputs": [], "source": ["lr.intercept_"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "BtBVcfrzU3fn"}, "outputs": [], "source": ["np.set_printoptions(8)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Y4KKd3bHU_hz", "outputId": "a333d015-2abd-4914-e81f-7e17f870f452"}, "outputs": [], "source": ["lr.coef_[lr.coef_!=0].shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "pDwOe637VDT4", "outputId": "36441242-6919-4da8-e547-8d8205988bc5"}, "outputs": [], "source": ["lr.coef_"]}, {"cell_type": "markdown", "metadata": {"id": "SDJP-RAKUwqb"}, "source": ["Let's vary the regularization strength and plot the regularization path - the weight coefficients of\n", "the different features for different reguarization strenghs: <br>\n", "Here C is the inverse of the regularization parameter lambda"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 444}, "id": "bvNcudiTVmoz", "outputId": "067b24bf-5955-49b7-9f53-d542cb39cfb8"}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "fig = plt.figure()\n", "ax = plt.subplot(111)\n", "colors = ['blue', 'green', 'red', 'cyan',\n", "          'magenta', 'yellow', 'black',\n", "          'pink', 'lightgreen', 'lightblue',\n", "          'gray', 'indigo', 'orange']\n", "weights, params = [], []\n", "for c in np.arange(-4., 6.):\n", "  lr = LogisticRegression(penalty='l1', C=10.**c, solver='liblinear',multi_class='ovr', random_state=0)\n", "  lr.fit(X_train_std, y_train)\n", "  weights.append(lr.coef_[1])\n", "  params.append(10**c)\n", "weights = np.array(weights)\n", "for column, color in zip(range(weights.shape[1]), colors):\n", "  plt.plot(params, weights[:, column],\n", "           label=df_wine.columns[column + 1],\n", "           color=color)\n", "plt.axhline(0, color='black', linestyle='--', linewidth=3)\n", "plt.xlim([10**(-5), 10**5])\n", "plt.ylabel('weight coefficient')\n", "plt.xlabel('C')\n", "plt.xscale('log')\n", "plt.legend(loc='upper left')\n", "ax.legend(loc='upper center',\n", "bbox_to_anchor=(1.38, 1.03),\n", "ncol=1, fancybox=True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "Yhd1G6KPXPhj"}, "source": ["**Query:** What do you observe from the plot? <br>\n", "All feature weights will be zero if we penalize the model with a strong regul arization parameter(C<0.1)"]}, {"cell_type": "markdown", "metadata": {"id": "W1dpzZOUXY0W"}, "source": ["# 6 Assessing feature importance with Random Forests"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 722}, "id": "EHYxwHY8Xbej", "outputId": "a544ceff-9cda-4307-daa8-365554f139c7"}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "feat_labels = df_wine.columns[1:]\n", "forest = RandomForestClassifier(n_estimators=500, random_state=1)\n", "forest.fit(X_train, y_train)\n", "importances = forest.feature_importances_\n", "indices = np.argsort(importances)[::-1]\n", "for f in range(X_train.shape[1]):\n", "  print(\"%2d) %-*s %f\" % (f + 1, 30,\n", "                          feat_labels[indices[f]],\n", "                          importances[indices[f]]))\n", "plt.title('Feature Importance')\n", "plt.bar(range(X_train.shape[1]), importances[indices], align='center')\n", "plt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation=90)\n", "plt.xlim([-1, X_train.shape[1]])\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "demp45BiZiZ5"}, "source": ["*Disclaimer*: The above code is modifed from the textbook \"Python Machine Learning\" by Sebastian Raschka."]}, {"cell_type": "markdown", "metadata": {"id": "HEYr-PZ5Z0Jk"}, "source": ["Discuss the following questions in groups:\n", "\n", "1.   Select the important features for model fitting from above plot and importance values\n", "2.   Get three separate models by combining six, five and four best features into separate dataframes\n", "3. Identify training and testing accuracy for them.\n", "4. Plot paired training and testing accuracy for the models.\n", "\n"]}], "metadata": {"colab": {"collapsed_sections": ["2Vhgyz4WTk6j"], "provenance": []}, "kernelspec": {"display_name": "Python (env1)", "language": "python", "name": "env1"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 0}
