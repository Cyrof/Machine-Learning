{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Preprocessing \n", "\n", "### Import library"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd \n", "import re\n", "import string\n", "import os\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.tokenize import word_tokenize\n", "from nltk.stem import WordNetLemmatizer\n", "\n", "# Download NLTK data files\n", "try:\n", "    nltk.data.find('tokenizers/punkt')\n", "    nltk.data.find('corpora/stopwords')\n", "    nltk.data.find('corpora/wordnet')\n", "except LookupError:\n", "    nltk.download('punkt')\n", "    nltk.download('stopwords')\n", "    nltk.download('wordnet')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load csv file into dataframe \n", "DATADIR = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/dataset\"\n", "main_df = pd.read_csv(f\"{DATADIR}/trump_insults_tweets.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Drop unnecessary columns\n", "main_df = main_df.drop(columns=main_df.columns[0])\n", "\n", "# Convert 'data' column to datetime\n", "main_df['date'] = pd.to_datetime(main_df['date'])\n", "\n", "# Combine all tweets into a single string \n", "all_tweets = ' '.join(main_df['tweet'])\n", "\n", "main_df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function to clean text \n", "def clean_text(text):\n", "    # convert text to lowercase\n", "    text = text.lower()\n", "\n", "    # remove punctuation\n", "    text = text.translate(str.maketrans('', '', string.punctuation))\n", "\n", "    # Remove numbers\n", "    text = re.sub(r'\\d+', '', text)\n", "\n", "    # tokenize the text\n", "    tokens = word_tokenize(text)\n", "\n", "    # remove stopwords \n", "    stop_words = set(stopwords.words('english'))\n", "    tokens = [word for word in tokens if word not in stop_words]\n", "\n", "    # Lemmatize the text\n", "    lemmatizer = WordNetLemmatizer()\n", "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n", "\n", "    # join tokens back into a single string \n", "    cleaned_text = ' '.join(tokens)\n", "\n", "    return cleaned_text"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Apply the cleaning function to each tweet\n", "main_df['cleaned_tweet'] = main_df['tweet'].apply(clean_text)\n", "\n", "# display the cleaned data\n", "main_df[['tweet', 'cleaned_tweet']].head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Save Cleaned Data for Top2Vec\n", "ONce the text is cleand, save it to a new CSV file for use with Top2Vec"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# save the cleaned tweets to a new CSV file \n", "main_df[['cleaned_tweet']].to_csv(f\"{DATADIR}/cleaned_tweets.csv\", index=False)"]}], "metadata": {"kernelspec": {"display_name": "assess2", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.4"}}, "nbformat": 4, "nbformat_minor": 2}
