{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Preprocessing \n", "\n", "In this section, preprocessing will be done to the dataset\n", "### Import library"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd \n", "import re\n", "import string\n", "import os\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.tokenize import word_tokenize\n", "from nltk.stem import WordNetLemmatizer\n", "\n", "# Download NLTK data files\n", "try:\n", "    nltk.data.find('tokenizers/punkt')\n", "    nltk.data.find('corpora/stopwords')\n", "    nltk.data.find('corpora/wordnet')\n", "except LookupError:\n", "    nltk.download('punkt')\n", "    nltk.download('stopwords')\n", "    nltk.download('wordnet')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Import Dataset into DataFrame"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load csv file into dataframe \n", "DATADIR = f\"{os.path.abspath(os.path.join(os.getcwd(), os.pardir))}/dataset\"\n", "main_df = pd.read_csv(f\"{DATADIR}/trump_insults_tweets.csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Initial Data Cleaning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Drop unnecessary columns\n", "main_df = main_df.drop(columns=main_df.columns[0])\n", "\n", "# Convert 'data' column to datetime\n", "main_df['date'] = pd.to_datetime(main_df['date'])\n", "\n", "# Combine all tweets into a single string \n", "all_tweets = ' '.join(main_df['tweet'])\n", "\n", "main_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Define Text Cleaning Function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function to clean text \n", "def clean_text(text):\n", "    # convert text to lowercase\n", "    text = text.lower()\n", "\n", "    # remove punctuation\n", "    text = text.translate(str.maketrans('', '', string.punctuation))\n", "\n", "    # Remove numbers\n", "    text = re.sub(r'\\d+', '', text)\n", "\n", "    # tokenize the text\n", "    tokens = word_tokenize(text)\n", "\n", "    # remove stopwords \n", "    stop_words = set(stopwords.words('english'))\n", "    tokens = [word for word in tokens if word not in stop_words]\n", "\n", "    # Lemmatize the text\n", "    lemmatizer = WordNetLemmatizer()\n", "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n", "\n", "    # join tokens back into a single string \n", "    cleaned_text = ' '.join(tokens)\n", "\n", "    return cleaned_text"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Apply Cleaning Function to Tweets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Apply the cleaning function to each tweet\n", "main_df['cleaned_tweet'] = main_df['tweet'].apply(clean_text)\n", "\n", "# display the cleaned data\n", "main_df[['tweet', 'cleaned_tweet']].head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Save Cleaned Data for Top2Vec\n", "Once the text is cleand, save it to a new CSV file for use with Top2Vec"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# save the cleaned tweets to a new CSV file \n", "main_df[['cleaned_tweet']].to_csv(f\"{DATADIR}/cleaned_tweets.csv\", index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Feature Extraction\n", "In this section, feature extraction will be done using Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n", "### Bag of words (BoW)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "\n", "# Create the CountVectorizer() \n", "vectorizer = CountVectorizer()\n", "\n", "# Fit and transform the cleaned tweets \n", "X_bow = vectorizer.fit_transform(main_df['cleaned_tweet'])\n", "\n", "# Convert to DataFrame for better visualisation \n", "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n", "bow_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Checking the BoW DataFrame \n", "To further check the BoW DataFrame, you can examine specific tweets and their corresponding word counts."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# display the first tweet and its BoW representation \n", "main_df['cleaned_tweet'].iloc[0]\n", "bow_df.iloc[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# display the second tweet and its BoW representation \n", "main_df['cleaned_tweet'].iloc[1]\n", "bow_df.iloc[1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Summarising Word Frequencies "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sum the coutns for each word across all tweets \n", "word_frequencies = bow_df.sum(axis=0)\n", "\n", "# display t he top 10 most frequent words \n", "word_frequencies.nlargest(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualising Word Frequencies "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# plot the top 10 most frequent words \n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "top_words = word_frequencies.nlargest(10).reset_index()\n", "top_words.columns = ['word', 'frequency']\n", "\n", "plt.figure(figsize=(12, 6))\n", "sns.barplot(x='frequency', y='word', data=top_words, palette='viridis', hue='frequency')\n", "plt.title('Top 10 Most Frequent Words')\n", "plt.xlabel('Frequency')\n", "plt.ylabel('Word')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Term Frequency-Inverse Document Frequency (TF-IDF)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "# Create the TfidfVectorizer()\n", "tfidf_vectorizer = TfidfVectorizer()\n", "\n", "# fit and trasform the cleaned tweets\n", "X_tfidf = tfidf_vectorizer.fit_transform(main_df['cleaned_tweet'])\n", "\n", "# Convert to DataFrame for better visualisation \n", "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n", "tfidf_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Summarising TF-IDF Scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sum the TF-IDF scores for each term \n", "tfidf_scores = tfidf_df.sum(axis=0)\n", "\n", "# Get the top 10 terms with the highest TF-IDF scores \n", "top_tfidf = tfidf_scores.nlargest(10).reset_index()\n", "top_tfidf.columns = ['term', 'score']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualise the top TF-IDF scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 6))\n", "sns.barplot(x='score', y='term', data=top_tfidf, palette='viridis', hue='score')\n", "plt.title('Top 10 Terms by TF-IDF Score')\n", "plt.xlabel('TF-IDF Score')\n", "plt.ylabel('Terms')\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "assess2", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.4"}}, "nbformat": 4, "nbformat_minor": 2}
